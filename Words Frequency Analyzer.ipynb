{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import required modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import wordpunct_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "import nltk\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read comments from local csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comments_df = pd.read_csv('output.csv', index_col=0, engine='python')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define function to clean comment i.e to remove emojis and punctuations "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_comment(comment):       \n",
    "        return ' '.join(re.sub(\"(@[A-Za-z0-9]+)|([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)\", \" \", comment).split())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean comments in dataframe and create a list of all the words in comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comments_df['Comment'] = comments_df['Comment'].apply(clean_comment)\n",
    "\n",
    "tokens = [wordpunct_tokenize(comment) for comment in comments_df['Comment']] \n",
    "words = [word.lower() for token in tokens for word in token]\n",
    "\n",
    "words_set = [word for word in words if word not in stopwords.words('english')]\n",
    "words_set = [word for word in words_set if len(word) > 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create frequency distribution of words and find out most common words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq = nltk.FreqDist(words_set)\n",
    "\n",
    "final_words = []\n",
    "frequency = []\n",
    "for key, value in freq.most_common(10):\n",
    "    final_words.append(key)\n",
    "    frequency.append(value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(final_words)):\n",
    "    print(final_words[i] + ' - ' + str(frequency[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# languages_ratios = {}\n",
    "\n",
    "# #print(words)\n",
    "\n",
    "# for language in stopwords.fileids():\n",
    "#     stopwords_set = set(stopwords.words(language))\n",
    "#     words_set = set(words)\n",
    "#     common_elements = words_set.intersection(stopwords_set)\n",
    "\n",
    "#     languages_ratios[language] = len(common_elements)\n",
    "#     #print(language + \": \" + str(common_elements))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# most_rated_language = max(languages_ratios, key=languages_ratios.get)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stopwords_english = set(stopwords.words('english'))\n",
    "# stopwords_hinglish = set(stopwords.words('hinglish'))\n",
    "# common_elements = stopwords_english.intersection(stopwords_hinglish)\n",
    "# print(len(stopwords_english))\n",
    "# print(len(stopwords_hinglish))\n",
    "# print(len(common_elements))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stemmer = nltk.stem.snowball.SnowballStemmer('english')\n",
    "# print(words_set)\n",
    "# words_set = [stemmer.stem(word) for word in words_set]\n",
    "# print(words_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x = [i for i in range(len(final_words))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig, ax = plt.subplots(nrows=5, ncols=1, figsize=(20,50))\n",
    "# ax[0].bar(x[0:20], frequency[0:20])\n",
    "# ax[0].set_xticks(x[0:20])\n",
    "# ax[0].set_xticklabels(final_words[0:20])\n",
    "\n",
    "# ax[1].bar(x[20:40], frequency[20:40])\n",
    "# ax[1].set_xticks(x[20:40])\n",
    "# ax[1].set_xticklabels(final_words[20:40])\n",
    "\n",
    "# ax[2].bar(x[40:60], frequency[40:60])\n",
    "# ax[2].set_xticks(x[40:60])\n",
    "# ax[2].set_xticklabels(final_words[40:60])\n",
    "\n",
    "# ax[3].bar(x[60:80], frequency[60:80])\n",
    "# ax[3].set_xticks(x[60:80])\n",
    "# ax[3].set_xticklabels(final_words[60:80])\n",
    "\n",
    "# ax[4].bar(x[80:100], frequency[80:100])\n",
    "# ax[4].set_xticks(x[80:100])\n",
    "# ax[4].set_xticklabels(final_words[80:100])\n",
    "# plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
